:toc:

= Red Hat Tech Exchange 2019


== A2003 Hybrid Cloud Management - Cost Management on AWS


`rhte2019-maa-hcm-cost-aws`

Welcome to the 2019 HCM Cost Management with OpenShift and AWS Lab

Red Hat is in the midst of developing SaaS offerings for our customer base all available at link:https://cloud.redhat.com[^] and link:https://cloud.openshift.com[^]. One of the early offerings will be Hybrid Cloud Cost Management. It is an Open Source project called "Koku" hosted at link:https://github.com/project-koku/[^] with documentation at link:https://koku.readthedocs.io/en/latest/[^]

Cost Management reveals the true costs of Business Initiatives.  It does so by appropriately tagging and labeling the resources used by an initiative and offering appropriate reporting on them. Most cloud platforms offer hourly reports on the costs of resources consumed, and when properly labeled and tagged, analysts can report on them.   Further features like forecasting, price-markups, etc. are forthcoming.

To understand and sell this product, Red Hatters will need to learn the basic financial terminology and the problem domains that this product seeks to solve, including:

* Budgeting
* Forecasting
* Workload Optimization

=== Goals

In this lab you will:

* Walk through the setup of the Metering Operator on your own cluster
* Add your cluster as a "source" for cost related information to the Account
* Use labels and tags to associate OpenShift resources with AWS resource tags
* Analyse of the existing Account's current costs per Business Initiative
* Forecast the cost of the Business's proposed Initiatives

[WARNING]
Hybrid Cloud Cost Managment is in the *BETA* state.  Not all features are expected to work.  Your suggestions for improvements and features are welcome.


[NOTE]
*OCP Usage Data Upload*
Upon General Availability, HCCM will upload usage data via the separate project OpenShift Telemetry (link:https://docs.openshift.com/container-platform/4.1/telemetry/about-telemetry.html[OpenShift Telemetry^]).  Until the products are integrated and are GA, a sub-project called "katakura" is supplied by the Cost Management team to upload data from OpenShift to the HCCM SaaS.


=== Infrastructure and Accounts

* Your OCP4 Cluster - one small OCP4 cluster
** 1 Master, 3 workers, 1 bastion
** OpenShift Web Console: `admin` `rhte2019`
** Metering Operator already installed. Learn more at link:https://github.com/operator-framework/operator-metering[^]

* Your Bastion Host
** Bastion SSH: `lab-user` `rhte2019`
** OCP 4 Installer - already complete
** OCP `oc` client - `system:admin` keys setup
** Cost Collector (sub-project "katakura") already installed and configured

* Your AWS Sandbox Account
** CLI access ONLY (sorry)
** Details are custom to each student, via email
*** aws access key id
*** aws secret access key

* The Cost Management SaaS accounts served from link:https://cloud.redhat.com/hybrid/cost-management/[^] 
** Red Hat Tech Exchange - Management Business Unit Account 1 - Number: 6289400 (rhte19mbu1org)
*** Live data where you will add your clusters as a data source
** Account 2 - Number: 6289402 (rhte19mbu2org)
*** Static Sample data where you can see an example of Tags and Labels working together

=== Prerequisites

. Get a GUID from the GUIDGrabber. This is the GUID of your personal cluster.
* Cluster 1 has a bastion VM that you will use to execute all tasks.
* GUIDGrabber will show the command to connect to the bastion VM.
* Your cluster runs in a Sandbox. Make a note of the Sandbox Number.
** For example if GUIDGrabber shows the following command:
+
[source,sh]
----
ssh lab-user@bastion.b1fa.sandbox23.opentlc.com
----
+
this means that your GUID is b1fa and your Sandbox Number is 23.

== Examine Cost Management on AWS

Your cloud provider is the first stop in gathering cost information.  The Cost Management will support both AWS and Azure, with more platforms coming.  We'll focus on AWS.

AWS reports costs for consumption by third parties via a feature called "AWS Cost and Usage Reports."  The AWS root account can setup these reports, including the level of data detail and the AWS tags that are included. AWS generates CSV files and places them in special S3 buckets almost hourly.  Red Hat Cost Managemetn gathers these logs from the S3 bucket almost hourly and processes them.  Since you only have an AWS "linked" account, not a root account, you will not be able to alter the configuration of AWS cost reporting.  Sorry about that.  We'll do our best to show them to you.

Here's how they're setup in AWS and the Red Hat HCCM SaaS:

* Observe the Setup of AWS for Cost Management

:imagesdir: images/


. Turn on AWS Cost and Usage Reports
+

+++ <details><summary> +++
Click me!
+++ </summary><div> +++
image:01_aws_billing_console.png[]
+++ </div></details> +++

. There's already a Cost and Usage Report already setup for the *rhte19mbu1org*
+

+++ <details><summary> +++
+++ </summary><div> +++
image:02_aws_cost_and_usage_reports.png[]
+++ </div></details> +++

. AWS is generating the reports and putting them in a bucket named *rhte19mbu1org*
+

+++ <details><summary> +++
+++ </summary><div> +++
image:03_aws_report_details.png[]
+++ </div></details> +++

. AWS has already filled the bucket with some information for September 2019
+
+++ <details><summary> +++
+++ </summary><div> +++
image:04_aws_populated_bucket.png[]
+++ </div></details> +++

. To protect the bucket and allow only the HCCM SaaS access to the bucket, a policy is created.
+
+++ <details><summary> +++
+++ </summary><div> +++
image:05_aws_iam_policies.png[]
+++ </div></details> +++

. Red Hat HCCM accesses the bucket based on a strict access policy
+
+++ <details><summary> +++
+++ </summary><div> +++
image:06_aws_policy_detail.png[]
+++ </div></details> +++

. An AWS Role is created to join the policy governing bucket access with the HCCM root account as a Trust Relationship.
+
+++ <details><summary> +++
+++ </summary><div> +++
image:07_aws_role_with_policy.png[]
+++ </div></details> +++

. Finally, select AWS tags that will be used by AWS to report resource utilization.
+
+++ <details><summary> +++
+++ </summary><div> +++
image:08_aws_cost_allocation_tags.png[]
+++ </div></details> +++

. Now, changing to the HCCM "Cost Management Sources" GUI in  link:https://cloud.redhat.com/hybrid/cost-management/sources[^] and an AWS source by indicating the bucket name and role created above.
+
+++ <details><summary> +++
+++ </summary><div> +++
image:09_cost_aws_source_added.png[]
+++ </div></details> +++

. Within a few hours, the cloud tags should appear in the HCCM "Cloud Details" GUI.  You can then group your costs by these tags and begin getting insights into the cost of your business initiatives. link:https://cloud.redhat.com/hybrid/cost-management/aws?group_by[account]=*&order_by[cost]=desc
+
+++ <details><summary> +++
+++ </summary><div> +++
image:10_cost_aws_cloud_tags_available.png[]
+++ </div></details> +++

== Examine Cost Management on OCP4

* Examine how Cost Management is Deployed on OCP4

On the bastion host, use the `oc` tool to talk to the API and learn about the Metering Operator

. SSH from your laptop to the Bastion
+
[source]
----
$ ssh lab-user@<bastion>
----
+
NOTE: If you have trouble logging in, ask one of the lab assistants

. A dedicated namespace was create for OpenShift Metering
+
[source]
----
$ oc project openshift-metering
----
+
.Sample Output:
[source,text]
----
Now using project "openshift-metering" on server "https://api.shared.na.openshift.opentlc.com:6443".
----

. The Metering Operator was made available to the cluster via the Metering Catalog Source
+
[source]
----
$ oc get catalogsource -A
----
+
.Sample Output:
[source,options="nowrap"]
----
NAMESPACE                              NAME                       NAME                  TYPE       PUBLISHER   AGE
openshift-logging                      cluster-logging-operator   Custom                grpc       Custom      6d3h
openshift-marketplace                  certified-operators        Certified Operators   grpc       Red Hat     6d4h
openshift-marketplace                  community-operators        Community Operators   grpc       Red Hat     6d4h
openshift-marketplace                  redhat-operators           Red Hat Operators     grpc       Red Hat     6d4h
openshift-metering                     metering-operators         Custom                grpc       Custom      6d3h
openshift-operator-lifecycle-manager   olm-operators              OLM Operators         internal   Red Hat     6d4h
openshift-operators                    elasticsearch-operator     Custom                grpc       Custom      6d3h
----

. It needs an OLM OperatorGroup to define relationships between operators. (More OLM info link:https://docs.openshift.com/container-platform/4.1/applications/operators/olm-understanding-olm.html#olm-operatorgroups_olm-understanding-olm[here^].)
+
[source]
----
$ oc get operatorgroup metering-operators -n openshift-metering -oyaml
----
+

+++ <details><summary> +++
_Sample Output_
+++ </summary><div> +++
+
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  annotations:
    olm.providedAPIs: HiveTable.v1alpha1.metering.openshift.io,Metering.v1alpha1.metering.openshift.io,PrestoTable.v1alpha1.metering.openshift.io,Report.v1alpha1.metering.openshift.io,ReportDataSource.v1alpha1.metering.openshift.io,ReportQuery.v1alpha1.metering.openshift.io,StorageLocation.v1alpha1.metering.openshift.io
  creationTimestamp: 2019-09-03T21:42:54Z
  generation: 2
  name: metering-operators
  namespace: openshift-metering
  resourceVersion: "71746600"
  selfLink: /apis/operators.coreos.com/v1/namespaces/openshift-metering/operatorgroups/metering-operators
  uid: c998fe67-ce93-11e9-b5d9-0a16ab677b4c
spec:
  serviceAccount:
    metadata:
      creationTimestamp: null
  targetNamespaces:
  - openshift-metering
status:
  lastUpdated: 2019-09-03T21:42:54Z
  namespaces:
  - openshift-metering
----
+++ </div></details> +++


. The Metering Subscription is also part of the OLM and defines which version and channel
+
[source,bash]
----
$ oc get subscriptions.operators.coreos.com metering -n openshift-metering -oyaml
----
+++ <details><summary> +++
_Sample Output_
+++ </summary><div> +++
+
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  creationTimestamp: "2019-09-10T15:45:14Z"
  generation: 1
  labels:
    csc-owner-name: installed-community-openshift-metering
    csc-owner-namespace: openshift-marketplace
  name: metering
  namespace: openshift-metering
  resourceVersion: "20929"
  selfLink: /apis/operators.coreos.com/v1alpha1/namespaces/openshift-metering/subscriptions/metering
  uid: fb4f6b21-d3e1-11e9-9c86-06ae53090800
spec:
  channel: preview
  name: metering
  source: metering-operators
  sourceNamespace: openshift-metering
status:
  currentCSV: metering-operator.v4.1.0
  installPlanRef:
    apiVersion: operators.coreos.com/v1alpha1
    kind: InstallPlan
    name: install-ln82l
    namespace: openshift-metering
    resourceVersion: "20867"
    uid: 341e2c12-d3e2-11e9-8f8b-06ae53090800
  installedCSV: metering-operator.v4.1.0
  installplan:
    apiVersion: operators.coreos.com/v1alpha1
    kind: InstallPlan
    name: install-ln82l
    uuid: 341e2c12-d3e2-11e9-8f8b-06ae53090800
  lastUpdated: "2019-09-10T15:46:51Z"
  state: AtLatestKnown
----
+++ </div></details> +++

. Finally, we actually kicked off the Metering install by creating the Metering Custom Resource
+
[source]
----
$ oc describe meterings.metering.openshift.io operator-metering
----
+++ <details><summary> +++
_Sample Output_
+++ </summary><div> +++
+
[source]
----
Name:         operator-metering
Namespace:    openshift-metering
Labels:       <none>
Annotations:  <none>
API Version:  metering.openshift.io/v1alpha1
Kind:         Metering
Metadata:
  Creation Timestamp:  2019-09-03T17:32:17Z
  Generation:          6
  Resource Version:    1824854
  Self Link:           /apis/metering.openshift.io/v1alpha1/namespaces/openshift-metering/meterings/operator-metering
  UID:                 c6d01c80-ce70-11e9-ae9b-021aec9d41ee
Spec:
  Hdfs:
    Spec:
      Datanode:
        Resources: [ommitted]
      Namenode:
        Resources: [ommitted]
  Presto:
    Spec:
      Hive:
        Metastore:
          Resources: [omitted]
          Storage:
            Size:  10Gi
        Server:
          Resources:
[omitted]
      Presto:
        Coordinator:
          Resources: [omitted]
        Worker:
          Replicas:  1
          Resources: [omitted]
  Reporting - Operator:
    Spec:
      Auth Proxy:
        Cookie Seed:                    7091da5a0a374e4a92a9356c963e1690
        Delegate UR Ls Enabled:         true
        Enabled:                        true
        Subject Access Review Enabled:  true
      Resources: [omitted]
      Route:
        Enabled:  true
Status:
  Observed Version:  680107
Events:              <none>
----
+++ </div></details> +++

. After a while, check it out, there are pods in the Metering Namespace.  The metering operator is an implementation of hdfs, i.e. Hadoop.  *_Big Data_*
+
[source]
----
$ oc get pods -n openshift-metering
----
+
.Sample Output:
----
NAME                                  READY   STATUS    RESTARTS   AGE
hdfs-datanode-0                       1/1     Running   1          6d3h
hdfs-namenode-0                       1/1     Running   1          6d3h
hive-metastore-0                      1/1     Running   1          6d3h
hive-server-0                         1/1     Running   1          6d3h
metering-operator-698f55bb84-fx5zl    2/2     Running   2          4d16h
presto-coordinator-7c57b6dfb5-cndbx   1/1     Running   1          4d16h
presto-worker-69f6f8c587-697g4        1/1     Running   1          6d3h
reporting-operator-6b5fdc8b5c-29qnx   2/2     Running   3          6d3h
----

. The OCP Usage uploader created some reports in the reporting operator that was installed. They're prefixed with HCCM.
+
[source]
----
$ oc get reports
----
+
.Sample Output:
[source,options="nowrap"]
----
NAME                                            QUERY                                           SCHEDULE   RUNNING                  FAILED   LAST REPORT TIME       AGE
hccm-openshift-persistentvolumeclaim            hccm-openshift-persistentvolumeclaim            hourly     ReportingPeriodWaiting            2019-09-09T21:00:00Z   6d3h
hccm-openshift-persistentvolumeclaim-lookback   hccm-openshift-persistentvolumeclaim-lookback   hourly     ReportingPeriodWaiting            2019-09-09T21:00:00Z   6d3h
hccm-openshift-usage                            hccm-openshift-usage                            hourly     ReportingPeriodWaiting            2019-09-09T21:00:00Z   6d3h
hccm-openshift-usage-lookback                   hccm-openshift-usage-lookback                   hourly     ReportingPeriodWaiting            2019-09-09T21:00:00Z   6d3h
----

. The metering operator has a lot of moving parts.  There are more things to try, if you like:
+
.Hadoop Queries
[source]
----
$ oc get reportqueries.metering.openshift.io
----
+
.Hadoop DataSources
[source]
----
$ oc get reportdatasources.metering.openshift.io
----

== Connect OCP and HCCM

To upload data that the metering operator has collected into the Cost Management SaaS, we'll be using the Koku sub-project link:https://github.com/project-koku/korekuta[korekuta^].  Korekuta has several moving parts:

. Some custom reports for the reporting-operator in the metering-operator
. Shell scripts to run the collector
. Ansible playbooks to keep the shell scripts a sane length
. Cronjobs to collect data periodically
. A configuration of the `insights-client`.  It first sets up reports and then periodically reads the reports from the metering operator and uploads them to cloud.redhat.com with the `insights-client`.

We've already set all this up for you on your environment. (You're welcome.) Let's have a look at its configuration.

. The `ocp_usage.sh` script keeps its configuration data in the filesystem of the bastion host.  The directory names under `$HOME/.config/ocp_usage/` are the cluster identifiers.
+
.Examine the Configs
[source,bash]
----
$ sudo -i
$ su - ec2-user
$ cat $HOME/.config/ocp_usage/*/config.json
----
+
.Sample Output:
[source,text]
----
{
    "ocp_api": "https://api.cluster-7371.7371.sandbox448.opentlc.com:6443", #<1>
    "ocp_token_file": "/home/ec2-user/7371.token", #<2>
    "ocp_cluster_id": "a1d4986f-eb03-57a9-bd1d-2ed6a9af4da0", #<3>
    "ocp_metering_namespace": "openshift-metering", #<4>
    "ocp_cli": "/usr/bin/oc", #<5>
    "ocp_validate_cert": "False", #<6>
    "metering_api": "https://metering-openshift-metering.apps.cluster-7371.7371.sandbox448.opentlc.com" #<7>
}
----
<1> The `ocp_usage.sh` collector will access the OpenShift cluster through the API endpoing.  Get it with `oc whoami --show-server`
<2> The token that belongs to the service account that was created to display reports. Get it with `oc serviceaccounts get-token reporting-operator -n openshift-metering`
<3> The cluster identifier used between the `ocp_usage.sh` scripts and the HCCM SaaS.  Also the name of the parent directory.
<4> The Metering Operator namespace.
<5> The `oc` command line tool appropriate for accessing this cluster.  Might need an `oc` client version 3 for older clusters.
<6> Certs are optional, though encouraged.
<7> The Route to the Reporting system to gather report to upload via `insights-client`.  Get it with `oc get route -n openshift-metering metering -o=jsonpath='{.status.ingress[0].host}'

. The korekuta source code is in `/home/ec2-user/korekuta-master/`

. There's a cronjob in the ec2-user's account:
+
[source,sh,options="nowrap"]
----
$ crontab -l
----
+
.Sample Output:
[source,sh,options="nowrap"]
----
#Ansible: korekuta
*/45 * * * * /home/ec2-user/korekuta-master/ocp_usage.sh --collect --e OCP_CLUSTER_ID=c4d465d8-6fea-5183-b1c3-e144b92d592d
----

* Add the Cluster through the GUI

. Navigate to link:https://cloud.redhat.com/hybrid/cost-management/sources[^]

. Sign in as username `rhte-example-1` password `r3dh4t1!`

. Click *Add Source*
.. *Name*: `ocp4-<your GUID>`.  For example `ocp4-3d0f`
.. *Type*: Select "*Red Hat OpenShift Container Platform*"
.. Click both the checkboxes.  They're already setup.
* [*] On your OpenShift cluster, install:
* [*] On a system with network access t your OpenShift cluster, install:
.. Click *Next*

. You already have the token. Click *Next*

. Paste the *Cluster Identifier* you got in the last step.  It's also in the output of the crontab.
.. Click *Next*

. The crontab is already setup for you.  Click *Next*

. Confirm the status details and click *Add Source*

. You will eventually see your cluster in the list of link:https://cloud.redhat.com/hybrid/cost-management/[*Top Clusters*^] or click on *All Clusters* to find yours.

* If data for your cluster hasn't already populated, wait a few hours for Korekuta and AWS to deliver reports and Koku to process them.  Report frequencies are by the hour.  Initial reports can take up to four hours to sync properly.

== Business Use Cases and Cost Management

=== Red Hat Global Partner and Technical Enablement: ELTs and ILTs

GPTE is in the business of delivering training. GPTE delivers both online training (ELT) and in-person training (ILT).

Let's create a system to track the cost of each student's resource usage in the cloud as they take classes.

=== Cloud Resources: Shared Clusters and Dedicated Environments

"Shared Clusters" are made up of resources shared with other students, on which they do their lab work. For example, students in a Shared Cluster are creating and deleting projects and associated OpenShift resources as part of their training.  Or perhaps, they might be sharing resources by pulling images from a common Quay registry.

"Dedicated Environments" are created for the student, and only the individual student has access to the resources. Oftentimes, these students are confined to a linked or "sandbox" account where they can create new cloud resources in a controlled fashion.

Classes can use Shared and/or Dedicated Resources to provide online environments to the students running labs as the lab creator sees fit. ELTs and ILTs can be taught by giving students access to a "Shared Cluster," or allowing the student to create new "Dedicated Environments".  Some use both "Shared Clusters" and "Dedicated Environments."

=== Cloud Tags and OpenShift Labels

.Default Values
By default, the Red Hat Cost Management service can detect which AWS EC2 instance IDs are being used by an OpenShift cluster.  This gives the user coarse grained information regarding the Cloud Resource consumption of the cluster.  This would be appropriate for the OCP-related costs of a student with a Dedicated Environment.  However, this does not give us precise knowledge of the students' activities in a Shared Cluster.

.Tags and Labels
To give us precise information as to the students' activities, GPTE needs a tagging system to ensure that the class lab environment that was used by the student is properly accounted for in the Cost Management system. As many as possible of the resourced need to be tagged or labeled, according to the features of the infrastructure providing them.

.Business Identifiers
Let's say that a student with ID `student1-redhat.com` is taking the *OpenShift 4 Foundations* ELT.  We need to label and tag all the resources they will be using for the course of the class.  We should choose a meaningful identifier for the student taking the class.  Let's say `class_session: student1-redhat.com_ocp4-foundations`

.Limitations
Each infrastructure system has its own limitations in their tagging and labeling mechanisms.  The total number of tags or labels in a system may be limited.  The number of tags on a particular resource may be limited.  The character count and allowed characters may differ.  Care must be taken to create tags and labels that suit all the systems involved.


== Business Analysis Scenario

GPTE Senior Management wants to know:

* How much have we spent, month by month, with AWS
* Infrastructure Cost per student to Run one OpenShift 4 ILT
* Infrastructure Cost per student to Run one OpenShift 4 "Foundations" ELT
* How many Students have done the OpenShift 4 "Foundations" ELT the past three months
* At current rate of usage increase, how much will we be spending on OpenShift 4 "Foundations" ELTs



